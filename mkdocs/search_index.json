{
    "docs": [
        {
            "location": "/", 
            "text": "ReverseDiff.jl\n\n\nTODO", 
            "title": "Introduction"
        }, 
        {
            "location": "/#reversediffjl", 
            "text": "TODO", 
            "title": "ReverseDiff.jl"
        }, 
        {
            "location": "/api/", 
            "text": "ReverseDiff API\n\n\n\n\nGradients of $f(x) : \\mathbb{R}^{n_1} \\times \\dots \\times \\mathbb{R}^{n_k} \\to \\mathbb{R}$\n\n\n#\n\n\nReverseDiff.gradient\n \n \nFunction\n.\n\n\nReverseDiff.gradient(f, input, cfg::GradientConfig = GradientConfig(input))\n\n\n\n\nIf \ninput\n is an \nAbstractArray\n, assume \nf\n has the form \nf(::AbstractArray{Real})::Real\n and return \n\u2207f(input)\n.\n\n\nIf \ninput\n is a tuple of \nAbstractArray\ns, assume \nf\n has the form \nf(::AbstractArray{Real}...)::Real\n (such that it can be called as \nf(input...)\n) and return a \nTuple\n where the \ni\nth element is the gradient of \nf\n w.r.t. \ninput[i].\n\n\nIt is highly recommended to preallocate and reuse \ncfg\n for subsequent calls.\n\n\nIf \nf\n contains no value-based control flow, it is highly recommended to use \nReverseDiff.GradientRecord\n to prerecord \nf\n. Otherwise, this method will have to re-record \nf\n's execution trace for every subsequent call.\n\n\nsource\n\n\n#\n\n\nReverseDiff.gradient!\n \n \nFunction\n.\n\n\nReverseDiff.gradient!(result, f, input, cfg::GradientConfig = GradientConfig(input))\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.gradient(f, input, cfg)\n, except it stores the resulting gradient(s) in \nresult\n rather than allocating new memory.\n\n\nresult\n can be an \nAbstractArray\n or a \nTuple\n of \nAbstractArray\ns. The \nresult\n (or any of its elements, if \nisa(result, Tuple)\n), can also be a \nDiffBase.DiffResult\n, in which case the primal value \nf(input)\n (or \nf(input...)\n, if \nisa(input, Tuple)\n) will be stored in it as well.\n\n\nsource\n\n\nReverseDiff.gradient!(rec::Union{GradientRecord,CompiledGradient}, input)\n\n\n\n\nIf \ninput\n is an \nAbstractArray\n, assume \nrec\n represents a function of the form \nf(::AbstractArray)::Real\n and return \n\u2207f(input)\n.\n\n\nIf \ninput\n is a tuple of \nAbstractArray\ns, assume \nrec\n represents a function of the form \nf(::AbstractArray...)::Real\n and return a \nTuple\n where the \ni\nth element is the gradient of \nf\n w.r.t. \ninput[i].\n\n\nsource\n\n\nReverseDiff.gradient!(result, rec::Union{GradientRecord,CompiledGradient}, input)\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.gradient!(rec, input)\n, except it stores the resulting gradient(s) in \nresult\n rather than allocating new memory.\n\n\nresult\n can be an \nAbstractArray\n or a \nTuple\n of \nAbstractArray\ns. The \nresult\n (or any of its elements, if \nisa(result, Tuple)\n), can also be a \nDiffBase.DiffResult\n, in which case the primal value \nf(input)\n (or \nf(input...)\n, if \nisa(input, Tuple)\n) will be stored in it as well.\n\n\nsource\n\n\n\n\nJacobians of $f(x) : \\mathbb{R}^{n_1} \\times \\dots \\times \\mathbb{R}^{n_k} \\to \\mathbb{R}^{m_1} \\times \\dots \\times \\mathbb{R}^{m_k}$\n\n\n#\n\n\nReverseDiff.jacobian\n \n \nFunction\n.\n\n\nReverseDiff.jacobian(f, input, cfg::JacobianConfig = JacobianConfig(input))\n\n\n\n\nIf \ninput\n is an \nAbstractArray\n, assume \nf\n has the form \nf(::AbstractArray{Real})::AbstractArray{Real}\n and return \nJ(f)(input)\n.\n\n\nIf \ninput\n is a tuple of \nAbstractArray\ns, assume \nf\n has the form \nf(::AbstractArray{Real}...)::AbstractArray{Real}\n (such that it can be called as \nf(input...)\n) and return a \nTuple\n where the \ni\nth element is the  Jacobian of \nf\n w.r.t. \ninput[i].\n\n\nIt is highly recommended to preallocate and reuse \ncfg\n for subsequent calls.\n\n\nIf \nf\n contains no value-based control flow, it is highly recommended to use \nReverseDiff.JacobianRecord\n to prerecord \nf\n. Otherwise, this method will have to re-record \nf\n's execution trace for every subsequent call.\n\n\nsource\n\n\nReverseDiff.jacobian(f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))\n\n\n\n\nExactly like \nReverseDiff.jacobian(f, input, cfg)\n, except the target function has the form \nf!(output::AbstractArray{Real}, input::AbstractArray{Real}...)\n.\n\n\nsource\n\n\n#\n\n\nReverseDiff.jacobian!\n \n \nFunction\n.\n\n\nReverseDiff.jacobian!(result, f, input, cfg::JacobianConfig = JacobianConfig(input))\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.jacobian(f, input, cfg)\n, except it stores the resulting Jacobian(s) in \nresult\n rather than allocating new memory.\n\n\nresult\n can be an \nAbstractArray\n or a \nTuple\n of \nAbstractArray\ns. The \nresult\n (or any of its elements, if \nisa(result, Tuple)\n), can also be a \nDiffBase.DiffResult\n, in which case the primal value \nf(input)\n (or \nf(input...)\n, if \nisa(input, Tuple)\n) will be stored in it as well.\n\n\nsource\n\n\nReverseDiff.jacobian!(result, f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))\n\n\n\n\nExactly like \nReverseDiff.jacobian!(result, f, input, cfg)\n, except the target function has the form \nf!(output::AbstractArray{Real}, input::AbstractArray{Real}...)\n.\n\n\nsource\n\n\nReverseDiff.jacobian!(rec::Union{JacobianRecord,CompiledJacobian}, input)\n\n\n\n\nIf \ninput\n is an \nAbstractArray\n, assume \nrec\n represents a function of the form \nf(::AbstractArray{Real})::AbstractArray{Real}\n or \nf!(::AbstractArray{Real}, ::AbstractArray{Real})\n and return \nrec\n's Jacobian w.r.t. \ninput\n.\n\n\nIf \ninput\n is a tuple of \nAbstractArray\ns, assume \nrec\n represents a function of the form \nf(::AbstractArray{Real}...)::AbstractArray{Real}\n or \nf!(::AbstractArray{Real}, ::AbstractArray{Real}...)\n and return a \nTuple\n where the \ni\nth element is \nrec\n's Jacobian w.r.t. \ninput[i].\n\n\nNote that if \nrec\n represents a function of the form \nf!(output, input...)\n, you can only execute \nrec\n with new \ninput\n values. There is no way to re-run \nrec\n's tape with new \noutput\n values; since \nf!\n can mutate \noutput\n, there exists no stable \"hook\" for loading new \noutput\n values into the tape.\n\n\nsource\n\n\nReverseDiff.jacobian!(result, rec::Union{JacobianRecord,CompiledJacobian}, input)\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.jacobian!(rec, input)\n, except it stores the resulting Jacobian(s) in \nresult\n rather than allocating new memory.\n\n\nresult\n can be an \nAbstractArray\n or a \nTuple\n of \nAbstractArray\ns. The \nresult\n (or any of its elements, if \nisa(result, Tuple)\n), can also be a \nDiffBase.DiffResult\n, in which case the primal value of the target function will be stored in it as well.\n\n\nsource\n\n\n\n\nHessians of $f(x) : \\mathbb{R}^{n_1} \\times \\dots \\times \\mathbb{R}^{n_k} \\to \\mathbb{R}$\n\n\n#\n\n\nReverseDiff.hessian\n \n \nFunction\n.\n\n\nReverseDiff.hessian(f, input::AbstractArray, cfg::HessianConfig = HessianConfig(input))\n\n\n\n\nGiven \nf(input::AbstractArray{Real})::Real\n, return \nf\ns Hessian w.r.t. to the given \ninput\n.\n\n\nIt is highly recommended to preallocate and reuse \ncfg\n for subsequent calls.\n\n\nIf \nf\n contains no value-based control flow, it is highly recommended to use \nReverseDiff.HessianRecord\n to prerecord \nf\n. Otherwise, this method will have to re-record \nf\n's execution trace for every subsequent call.\n\n\nsource\n\n\n#\n\n\nReverseDiff.hessian!\n \n \nFunction\n.\n\n\nReverseDiff.hessian!(result::AbstractArray, f, input::AbstractArray, cfg::HessianConfig = HessianConfig(input))\n\nReverseDiff.hessian!(result::DiffResult, f, input::AbstractArray, cfg::HessianConfig = HessianConfig(result, input))\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.hessian(f, input, cfg)\n, except it stores the resulting Hessian in \nresult\n rather than allocating new memory.\n\n\nIf \nresult\n is a \nDiffBase.DiffResult\n, the primal value \nf(input)\n and the gradient \n\u2207f(input)\n will be stored in it along with the Hessian \nH(f)(input)\n.\n\n\nsource\n\n\nReverseDiff.hessian!(rec::Union{HessianRecord,CompiledHessian}, input)\n\n\n\n\nAssuming \nrec\n represents a function of the form \nf(::AbstractArray{Real})::Real\n, return the Hessian \nH(f)(input)\n.\n\n\nsource\n\n\nReverseDiff.hessian!(result::AbstractArray, rec::Union{HessianRecord,CompiledHessian}, input)\n\nReverseDiff.hessian!(result::DiffResult, rec::Union{HessianRecord,CompiledHessian}, input)\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.hessian!(rec, input)\n, except it stores the resulting Hessian in \nresult\n rather than allocating new memory.\n\n\nIf \nresult\n is a \nDiffBase.DiffResult\n, the primal value \nf(input)\n and the gradient \n\u2207f(input)\n will be stored in it along with the Hessian \nH(f)(input)\n.\n\n\nsource", 
            "title": "ReverseDiff API"
        }, 
        {
            "location": "/api/#reversediff-api", 
            "text": "", 
            "title": "ReverseDiff API"
        }, 
        {
            "location": "/api/#gradients-of-fx-mathbbrn_1-times-dots-times-mathbbrn_k-to-mathbbr", 
            "text": "#  ReverseDiff.gradient     Function .  ReverseDiff.gradient(f, input, cfg::GradientConfig = GradientConfig(input))  If  input  is an  AbstractArray , assume  f  has the form  f(::AbstractArray{Real})::Real  and return  \u2207f(input) .  If  input  is a tuple of  AbstractArray s, assume  f  has the form  f(::AbstractArray{Real}...)::Real  (such that it can be called as  f(input...) ) and return a  Tuple  where the  i th element is the gradient of  f  w.r.t.  input[i].  It is highly recommended to preallocate and reuse  cfg  for subsequent calls.  If  f  contains no value-based control flow, it is highly recommended to use  ReverseDiff.GradientRecord  to prerecord  f . Otherwise, this method will have to re-record  f 's execution trace for every subsequent call.  source  #  ReverseDiff.gradient!     Function .  ReverseDiff.gradient!(result, f, input, cfg::GradientConfig = GradientConfig(input))  Returns  result . This method is exactly like  ReverseDiff.gradient(f, input, cfg) , except it stores the resulting gradient(s) in  result  rather than allocating new memory.  result  can be an  AbstractArray  or a  Tuple  of  AbstractArray s. The  result  (or any of its elements, if  isa(result, Tuple) ), can also be a  DiffBase.DiffResult , in which case the primal value  f(input)  (or  f(input...) , if  isa(input, Tuple) ) will be stored in it as well.  source  ReverseDiff.gradient!(rec::Union{GradientRecord,CompiledGradient}, input)  If  input  is an  AbstractArray , assume  rec  represents a function of the form  f(::AbstractArray)::Real  and return  \u2207f(input) .  If  input  is a tuple of  AbstractArray s, assume  rec  represents a function of the form  f(::AbstractArray...)::Real  and return a  Tuple  where the  i th element is the gradient of  f  w.r.t.  input[i].  source  ReverseDiff.gradient!(result, rec::Union{GradientRecord,CompiledGradient}, input)  Returns  result . This method is exactly like  ReverseDiff.gradient!(rec, input) , except it stores the resulting gradient(s) in  result  rather than allocating new memory.  result  can be an  AbstractArray  or a  Tuple  of  AbstractArray s. The  result  (or any of its elements, if  isa(result, Tuple) ), can also be a  DiffBase.DiffResult , in which case the primal value  f(input)  (or  f(input...) , if  isa(input, Tuple) ) will be stored in it as well.  source", 
            "title": "Gradients of $f(x) : \\mathbb{R}^{n_1} \\times \\dots \\times \\mathbb{R}^{n_k} \\to \\mathbb{R}$"
        }, 
        {
            "location": "/api/#jacobians-of-fx-mathbbrn_1-times-dots-times-mathbbrn_k-to-mathbbrm_1-times-dots-times-mathbbrm_k", 
            "text": "#  ReverseDiff.jacobian     Function .  ReverseDiff.jacobian(f, input, cfg::JacobianConfig = JacobianConfig(input))  If  input  is an  AbstractArray , assume  f  has the form  f(::AbstractArray{Real})::AbstractArray{Real}  and return  J(f)(input) .  If  input  is a tuple of  AbstractArray s, assume  f  has the form  f(::AbstractArray{Real}...)::AbstractArray{Real}  (such that it can be called as  f(input...) ) and return a  Tuple  where the  i th element is the  Jacobian of  f  w.r.t.  input[i].  It is highly recommended to preallocate and reuse  cfg  for subsequent calls.  If  f  contains no value-based control flow, it is highly recommended to use  ReverseDiff.JacobianRecord  to prerecord  f . Otherwise, this method will have to re-record  f 's execution trace for every subsequent call.  source  ReverseDiff.jacobian(f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))  Exactly like  ReverseDiff.jacobian(f, input, cfg) , except the target function has the form  f!(output::AbstractArray{Real}, input::AbstractArray{Real}...) .  source  #  ReverseDiff.jacobian!     Function .  ReverseDiff.jacobian!(result, f, input, cfg::JacobianConfig = JacobianConfig(input))  Returns  result . This method is exactly like  ReverseDiff.jacobian(f, input, cfg) , except it stores the resulting Jacobian(s) in  result  rather than allocating new memory.  result  can be an  AbstractArray  or a  Tuple  of  AbstractArray s. The  result  (or any of its elements, if  isa(result, Tuple) ), can also be a  DiffBase.DiffResult , in which case the primal value  f(input)  (or  f(input...) , if  isa(input, Tuple) ) will be stored in it as well.  source  ReverseDiff.jacobian!(result, f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))  Exactly like  ReverseDiff.jacobian!(result, f, input, cfg) , except the target function has the form  f!(output::AbstractArray{Real}, input::AbstractArray{Real}...) .  source  ReverseDiff.jacobian!(rec::Union{JacobianRecord,CompiledJacobian}, input)  If  input  is an  AbstractArray , assume  rec  represents a function of the form  f(::AbstractArray{Real})::AbstractArray{Real}  or  f!(::AbstractArray{Real}, ::AbstractArray{Real})  and return  rec 's Jacobian w.r.t.  input .  If  input  is a tuple of  AbstractArray s, assume  rec  represents a function of the form  f(::AbstractArray{Real}...)::AbstractArray{Real}  or  f!(::AbstractArray{Real}, ::AbstractArray{Real}...)  and return a  Tuple  where the  i th element is  rec 's Jacobian w.r.t.  input[i].  Note that if  rec  represents a function of the form  f!(output, input...) , you can only execute  rec  with new  input  values. There is no way to re-run  rec 's tape with new  output  values; since  f!  can mutate  output , there exists no stable \"hook\" for loading new  output  values into the tape.  source  ReverseDiff.jacobian!(result, rec::Union{JacobianRecord,CompiledJacobian}, input)  Returns  result . This method is exactly like  ReverseDiff.jacobian!(rec, input) , except it stores the resulting Jacobian(s) in  result  rather than allocating new memory.  result  can be an  AbstractArray  or a  Tuple  of  AbstractArray s. The  result  (or any of its elements, if  isa(result, Tuple) ), can also be a  DiffBase.DiffResult , in which case the primal value of the target function will be stored in it as well.  source", 
            "title": "Jacobians of $f(x) : \\mathbb{R}^{n_1} \\times \\dots \\times \\mathbb{R}^{n_k} \\to \\mathbb{R}^{m_1} \\times \\dots \\times \\mathbb{R}^{m_k}$"
        }, 
        {
            "location": "/api/#hessians-of-fx-mathbbrn_1-times-dots-times-mathbbrn_k-to-mathbbr", 
            "text": "#  ReverseDiff.hessian     Function .  ReverseDiff.hessian(f, input::AbstractArray, cfg::HessianConfig = HessianConfig(input))  Given  f(input::AbstractArray{Real})::Real , return  f s Hessian w.r.t. to the given  input .  It is highly recommended to preallocate and reuse  cfg  for subsequent calls.  If  f  contains no value-based control flow, it is highly recommended to use  ReverseDiff.HessianRecord  to prerecord  f . Otherwise, this method will have to re-record  f 's execution trace for every subsequent call.  source  #  ReverseDiff.hessian!     Function .  ReverseDiff.hessian!(result::AbstractArray, f, input::AbstractArray, cfg::HessianConfig = HessianConfig(input))\n\nReverseDiff.hessian!(result::DiffResult, f, input::AbstractArray, cfg::HessianConfig = HessianConfig(result, input))  Returns  result . This method is exactly like  ReverseDiff.hessian(f, input, cfg) , except it stores the resulting Hessian in  result  rather than allocating new memory.  If  result  is a  DiffBase.DiffResult , the primal value  f(input)  and the gradient  \u2207f(input)  will be stored in it along with the Hessian  H(f)(input) .  source  ReverseDiff.hessian!(rec::Union{HessianRecord,CompiledHessian}, input)  Assuming  rec  represents a function of the form  f(::AbstractArray{Real})::Real , return the Hessian  H(f)(input) .  source  ReverseDiff.hessian!(result::AbstractArray, rec::Union{HessianRecord,CompiledHessian}, input)\n\nReverseDiff.hessian!(result::DiffResult, rec::Union{HessianRecord,CompiledHessian}, input)  Returns  result . This method is exactly like  ReverseDiff.hessian!(rec, input) , except it stores the resulting Hessian in  result  rather than allocating new memory.  If  result  is a  DiffBase.DiffResult , the primal value  f(input)  and the gradient  \u2207f(input)  will be stored in it along with the Hessian  H(f)(input) .  source", 
            "title": "Hessians of $f(x) : \\mathbb{R}^{n_1} \\times \\dots \\times \\mathbb{R}^{n_k} \\to \\mathbb{R}$"
        }
    ]
}